# awesome-foundation-model-ros

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A collection of ROS projects and resources utilizing or simplifying the application of foundation models in robotics.

# Frameworks

ROS frameworks for embodied intelligence applications.

## Community-Ready Frameworks

- [RAI](https://github.com/RobotecAI/rai) - Flexible AI agent framework to develop and deploy Embodied AI features for robots.
- [ROSA](https://github.com/nasa-jpl/rosa) - AI Agent that can interact with ROS1- and ROS2-based robotics systems using natural language queries.

## Research-Grade Frameworks

- [ROS-LLM: A ROS framework for embodied AI with task feedback and structured reasoning](https://github.com/huawei-noah/HEBO/tree/rosllm/ROSLLM)
- [ROS-LLM (Auromix)](https://github.com/Auromix/ROS-LLM)
- [ROSGPT: Next-Generation Human-Robot Interaction with ChatGPT and ROS](https://github.com/aniskoubaa/rosgpt)
- [ROSGPT_Vision: Commanding Robots Using Only Language Models' Prompts](https://github.com/bilel-bj/ROSGPT_Vision)
- [The Conversation is the Command: Interacting with Real-World Autonomous Robots through Natural Language (TCC-IRoNL)](https://github.com/LinusNEP/TCC-IRoNL)

# Model Wrappers

ROS projects that wraps foundation models with a ROS interface.

- [llama_ros](https://github.com/mgonzs13/llama_ros) - ROS 2 wrapper for llama.cpp.
- [whisper_ros](https://github.com/mgonzs13/whisper_ros) - ROS 2 wrapper for whisper.cpp. Also provides Voice Activity Detection.
- [ros2_whisper](https://github.com/ros-ai/ros2_whisper) - ROS 2 wrapper for whisper.cpp.
- [ros2_sam](https://github.com/ros-ai/ros2_sam) ROS 1 wrapper for Segment Anything.
- [ros_sam](https://github.com/robot-learning-freiburg/ros_sam) - ROS 2 wrapper for Segment Anything.
- [grounding_sam_ros](https://github.com/HashimHS/grounding_sam_ros) - ROS 1 wrapper for Grounded Segment Anything.
- [isaac_ros_segment_anything](https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_image_segmentation/tree/main/isaac_ros_segment_anything) - ROS 2 wrapper for Segment Anything.
- [isaac_ros_foundationpose](https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_pose_estimation/tree/main/isaac_ros_foundationpose) - ROS 2 wrapper for FoundationPose.
- [isaac_ros_dnn_stereo_depth](https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_dnn_stereo_depth) - ROS 2 wrapper for Efficient Semi-Supervised stereo disparity (ESS).
- [ros2_nanollm](https://github.com/NVIDIA-AI-IOT/ros2_nanollm) - ROS 2 wrapper for streaming VLMs with quantization.
- [ROS2-NanoOWL](https://github.com/NVIDIA-AI-IOT/ROS2-NanoOWL) - ROS 2 wrapper for OWL-ViT open vocabulary detection with TensorRT.
- [visualnav-transformer-ros2](https://github.com/RobotecAI/visualnav-transformer-ros2) - ROS 2 port of [visualnav-transformer](https://github.com/robodhruv/visualnav-transformer) stack (GNM, ViNT and NoMaD)

# Developer Tools

Tools that can aid in the development, operation, and troubleshooting processes of robots.

- [ros2ai](https://github.com/fujitatomoya/ros2ai) - Use ROS 2 CLI through natural language.
- [ROSA](https://github.com/nasa-jpl/rosa) - Interact with ROS1- and ROS2-based robotics systems using natural language queries.
- [ROScribe](https://github.com/RoboCoachTechnologies/ROScribe) - Create ROS packages using LLMs.
- [explainable_ROS](https://github.com/Dsobh/explainable_ROS) - Generate explanations for the actions performed by an autonomous robot.

# Demo Projects

Demonstrates interesting usage foundation models in robots.

- [Tabletop Handybot](https://github.com/ycheng517/tabletop-handybot) - Use foundation models to perform basic tabletop tasks with a robotic arm.
- [Utilizing LLMs as a Task Planning Agent for Robotics](https://github.com/hlfshell/wpi-capstone) - Use LLMs for task planning for in-home item retrieval by a mobile robot.
- [rai-rosbot-xl-demo](https://github.com/RobotecAI/rai-rosbot-xl-demo) - Use the RAI framework to control a ROSbot XL mobile robot.
- [rai-agriculture-demo](https://github.com/RobotecAI/rai-agriculture-demo) - Use the RAI framework to supervise autonomous tractors.
- [remembr](https://github.com/NVIDIA-AI-IOT/remembr) - Building and Reasoning Over Long-Horizon Spatio-Temporal Memory for Robots
- [Yell At Your Robot](https://yay-robot.github.io/) - Improving On-the-Fly from Language Corrections
- [Physical Intelligence Ï€0](https://www.physicalintelligence.company/blog/pi0) - A Vision-Language-Action Flow Model for General Robot Control
  - Initial efforts are ongoing in the community to implement and train [pi-zero-pytorch](https://github.com/lucidrains/pi-zero-pytorch) and its precursor [Transfusion](https://github.com/lucidrains/transfusion-pytorch)
  - Contact [`@dusty-nv`](https://github.com/dusty-nv) if you are interested in participating or contributing data (see [`pi-zero-pytorch/issues/2`](https://github.com/lucidrains/pi-zero-pytorch/issues/2)])

# Awesome Foundation Models For Robotics

Awesome lists of foundation models that can be used in robotics.

- [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM) - A curated list of Large Language Models.
- [Awesome VLM Architectures](https://github.com/gokayfem/awesome-vlm-architectures) - A curated list of famous vision language models and their architectures.
- [Awesome Robotics Foundation Models](https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models) - A survey of foundation models in robotics.
- [Awesome Foundation and Multimodal Models](https://github.com/SkalskiP/awesome-foundation-and-multimodal-models) - Curated list of top foundation and multimodal models

# Adjacent communities to connect with:

- [LeRobot](https://github.com/huggingface/lerobot)
- [Jetson AI Lab](https://www.jetson-ai-lab.com/)
